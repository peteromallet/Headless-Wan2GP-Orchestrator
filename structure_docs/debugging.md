# Debugging Guide

## ðŸŽ¯ Quick Start: Use the Unified Debug Tool

All debugging functionality is consolidated into **one tool**: `scripts/debug.py`

**ðŸ“– Full documentation: [`scripts/debug/README.md`](../scripts/debug/README.md)**

## Essential Commands

```bash
# Investigate a failed task
python scripts/debug.py task <task_id>

# Check worker health
python scripts/debug.py worker <worker_id>
python scripts/debug.py worker <worker_id> --check-logging  # Is it running?
python scripts/debug.py worker <worker_id> --startup        # View initialization

# System overview
python scripts/debug.py health          # Overall health
python scripts/debug.py workers         # Recent worker failures
python scripts/debug.py tasks           # Recent task statistics
python scripts/debug.py orchestrator    # Is orchestrator alive?

# Configuration & Cost Management
python scripts/debug.py config --explain    # View all settings
python scripts/debug.py runpod              # Find orphaned pods ($$$ waste!)
```

## Common Scenarios

| Problem | Command |
|---------|---------|
| "Why did this task fail?" | `debug.py task <task_id>` |
| "Worker isn't doing anything" | `debug.py worker <id> --check-logging` |
| "System seems broken" | `debug.py health` |
| "Are we wasting money?" | `debug.py runpod` |
| "What are my timeout settings?" | `debug.py config --explain` |

## Architecture

The unified debug tool uses a **logs-first approach**:
- Primary data source: `system_logs` table (complete event timeline)
- Augmented with current state from `tasks` and `workers` tables
- Consistent formatting across all commands
- Single unified data access layer (`DebugClient`)

See [`scripts/debug/README.md`](../scripts/debug/README.md) for complete documentation.

---

## Other Tools (Specialized Use)

### Management Scripts
- `spawn_gpu.py` - Manually spawn a worker
- `shutdown_all_workers.py` - Emergency shutdown
- `terminate_single_worker.py` - Terminate specific worker

### Forensic Investigation (One-off Analysis)
- `investigate_*.py` - Specific incident investigations
- `forensic_analysis_*.py` - Historical failure analysis
- `reconstruct_from_worker_metadata.py` - Timeline reconstruction

### Testing & Setup
- `test_*.py` - Component testing
- `setup_database.py` - Database initialization
- `apply_sql_migrations.py` - Schema updates

### Monitoring
- `dashboard.py` - Real-time monitoring dashboard
- `monitor_worker.py` - Continuous worker monitoring

---

## Troubleshooting: Worker Health Checks

### How Health Checks Work

The orchestrator performs health checks on **idle workers** (workers with no active tasks and no queued work):

1. **Heartbeat Check** (Primary): Workers send heartbeats every 20 seconds via the guardian process
   - âœ… Heartbeat < 60s old â†’ Worker is healthy
   - âŒ Heartbeat > 60s old or missing â†’ Worker is unhealthy

2. **Failsafe Check**: Catches very stale workers (>idle timeout) regardless of status

**Important:** The system relies entirely on heartbeat for health detection. If a worker is heartbeating, it's alive and working - no SSH or network checks are performed to avoid false positives.

### Common False Positive (Fixed Nov 2025)

**Issue:** Workers were being terminated despite being healthy and actively processing tasks.

**Root Cause:** The old health check used SSH connectivity tests which failed due to temporary network issues (EAGAIN errors), even though the worker was still heartbeating.

**Fix:** Health checks now rely solely on heartbeat. If a worker is sending heartbeats (updated < 60s ago), it passes the health check. This eliminates false positives from network hiccups while still detecting truly dead workers.

---

**ðŸ’¡ Pro Tip:** Start with `debug.py health` for a quick system overview, then drill down into specific tasks or workers as needed.
